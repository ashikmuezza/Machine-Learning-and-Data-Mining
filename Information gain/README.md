# Information gain
Information gain calculates the reduction in entropy or surprise from transforming a dataset in some way. It can also be used for feature selection, by evaluating the gain of each variable in the context of the target variable. In this slightly different usage, the calculation is referred to as mutual information between the two random variables.

1. Information gain is the reduction in entropy or surprise by transforming a dataset and is often used in training decision trees.. 

2. Information gain is calculated by comparing the entropy of the dataset before and after a transformation. 

3. Mutual information calculates the statistical dependence between two variables and is the name given to information gain when applied to variable selection..



